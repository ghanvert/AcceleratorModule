

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>accmt.trainer &mdash; accmt  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            accmt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">accmt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">accmt.trainer</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for accmt.trainer</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2025 ghanvert. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">signal</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Mapping</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">nullcontext</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">DistributedType</span>
<span class="kn">from</span> <span class="nn">accelerate.utils</span> <span class="kn">import</span> <span class="n">ProjectConfiguration</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">LRScheduler</span>
<span class="kn">from</span> <span class="nn">torch.optim.optimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="kn">from</span> <span class="nn">.callbacks</span> <span class="kn">import</span> <span class="n">Callback</span><span class="p">,</span> <span class="n">CallbackMaster</span>
<span class="kn">from</span> <span class="nn">.dist_utils</span> <span class="kn">import</span> <span class="n">Gatherer</span><span class="p">,</span> <span class="n">rprint</span><span class="p">,</span> <span class="n">time_prefix</span>
<span class="kn">from</span> <span class="nn">.hyperparameters</span> <span class="kn">import</span> <span class="n">HyperParameters</span>
<span class="kn">from</span> <span class="nn">.metrics</span> <span class="kn">import</span> <span class="n">Metric</span>
<span class="kn">from</span> <span class="nn">.model_wrapper</span> <span class="kn">import</span> <span class="n">_DistributedDataParallel</span>
<span class="kn">from</span> <span class="nn">.modules</span> <span class="kn">import</span> <span class="n">AcceleratorModule</span>
<span class="kn">from</span> <span class="nn">.monitor</span> <span class="kn">import</span> <span class="n">Monitor</span>
<span class="kn">from</span> <span class="nn">.states</span> <span class="kn">import</span> <span class="n">LossState</span><span class="p">,</span> <span class="n">TrainingState</span>
<span class="kn">from</span> <span class="nn">.tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">.tracker</span> <span class="kn">import</span> <span class="n">_tracker_map</span>
<span class="kn">from</span> <span class="nn">.tunnel</span> <span class="kn">import</span> <span class="n">AsyncDiskQueue</span><span class="p">,</span> <span class="n">AsyncState</span><span class="p">,</span> <span class="n">ModelTunnel</span>
<span class="kn">from</span> <span class="nn">.utility</span> <span class="kn">import</span> <span class="n">ASYNC</span><span class="p">,</span> <span class="n">ASYNC_HASH</span><span class="p">,</span> <span class="n">ASYNC_TRAIN_GROUP</span><span class="p">,</span> <span class="n">DEBUG_MODE</span><span class="p">,</span> <span class="n">MASTER_PROCESS</span><span class="p">,</span> <span class="n">WORLD_SIZE</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">cleanup</span><span class="p">,</span>
    <span class="n">filter_kwargs</span><span class="p">,</span>
    <span class="n">get_number_and_unit</span><span class="p">,</span>
    <span class="n">get_seed</span><span class="p">,</span>
    <span class="n">is_url</span><span class="p">,</span>
    <span class="n">operator_map</span><span class="p">,</span>
    <span class="n">print_gpu_users_by_device</span><span class="p">,</span>
    <span class="n">set_seed</span><span class="p">,</span>
<span class="p">)</span>


<span class="n">CHECKPOINT_DIR</span> <span class="o">=</span> <span class="s2">&quot;checkpoint&quot;</span>
<span class="n">STATE_FILE</span> <span class="o">=</span> <span class="s2">&quot;state.json&quot;</span>
<span class="n">TRAIN_LOSS_STATE_FILE</span> <span class="o">=</span> <span class="s2">&quot;train_loss_state.pt&quot;</span>
<span class="n">_bar_format</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{l_bar}{bar}</span><span class="s2">| </span><span class="si">{n_fmt}</span><span class="s2">/</span><span class="si">{total_fmt}</span><span class="s2"> - ETA: </span><span class="si">{remaining}{postfix}</span><span class="s2"> - </span><span class="si">{rate_s}</span><span class="s2">&quot;</span>
<span class="n">_tqdm_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;leave&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;ncols&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;bar_format&quot;</span><span class="p">:</span> <span class="n">_bar_format</span><span class="p">}</span>


<div class="viewcode-block" id="Trainer">
<a class="viewcode-back" href="../../api.html#accmt.Trainer">[docs]</a>
<span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Class to implement full training process.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="Trainer.__init__">
<a class="viewcode-back" href="../../api.html#accmt.Trainer.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hps_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">HyperParameters</span><span class="p">],</span>
        <span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">track_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">multiple_checkpoints</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_checkpoints</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">resume</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">disable_model_saving</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">patience</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">evaluate_every_n_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">checkpoint_every</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
        <span class="n">logging_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;logs&quot;</span><span class="p">,</span>
        <span class="n">log_with</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">log_every</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">grad_accumulation_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">gradient_checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">gradient_checkpointing_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clip_grad</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">shuffle_train</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">sampler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">list</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">collate_fn_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">collate_fn_val</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_shard_size</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;10GB&quot;</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="nb">compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">compile_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">train_loss_metric_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train_loss&quot;</span><span class="p">,</span>
        <span class="n">val_loss_metric_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
        <span class="n">dataloader_pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dataloader_num_workers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dataloader_drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">eval_when_finish</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">eval_when_start</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">monitor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Monitor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Metric</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Metric</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Metric</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Metric</span><span class="p">]]]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cleanup_cache_every_n_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callback</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Callback</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Callback</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">additional_tracker_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_device_placement</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">prepare_batch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">safe_steps</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">destroy_after_training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">enable_prepare_logging</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Trainer constructor to set configuration.</span>

<span class="sd">        Args:</span>
<span class="sd">            hps_config (`str`, `dict`, or `HyperParameters`):</span>
<span class="sd">                YAML hyperparameters file path, dictionary or `HyperParameters`.</span>
<span class="sd">            model_path (`str`):</span>
<span class="sd">                Path to save model.</span>
<span class="sd">            track_name (`str`, *optional*, defaults to `None`):</span>
<span class="sd">                Track name for trackers. If set to `None` (default), the track name will be</span>
<span class="sd">                the model&#39;s folder name.</span>
<span class="sd">            enable_checkpointing (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Enable checkpointing.</span>
<span class="sd">            multiple_checkpoints (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Enable multiple checkpoints.</span>
<span class="sd">            max_checkpoints (`int`, *optional*, defaults to `None`):</span>
<span class="sd">                Maximum number of checkpoints to keep. If set to `None`, all checkpoints will be kept.</span>
<span class="sd">            resume (`bool` or `int`, *optional*, defaults to `None`):</span>
<span class="sd">                Whether to resume from checkpoint. Default option is `None`, which means resuming from checkpoint</span>
<span class="sd">                will be handled automatically, whether the checkpoint directory exists or not.</span>
<span class="sd">                If set to `True`, the latest checkpoint will be loaded.</span>
<span class="sd">                If set to an integer, the checkpoint will be loaded from the given index (if `multiple_checkpoints` is `True`).</span>
<span class="sd">                If set to `-1`, the latest checkpoint will be loaded (if `multiple_checkpoints` is `True`).</span>
<span class="sd">            disable_model_saving (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Disable any model saving registered (by default, `&quot;best_valid_loss&quot;` is registered, or if there are none evaluations to do,</span>
<span class="sd">                default will be `&quot;best_train_loss&quot;`).</span>
<span class="sd">            patience (`int` or `dict`, *optional*, defaults to `None`):</span>
<span class="sd">                Set up a patience parameter for model savings. If set, every model saving will check if the previous metric was higher.</span>
<span class="sd">                If the metric has not improved over the N model savings (`patience`), then the training process will stop. Can also</span>
<span class="sd">                implement patience per model saving in a dictionary.</span>
<span class="sd">            evaluate_every_n_steps (`int`, *optional*, defaults to `None`):</span>
<span class="sd">                Evaluate model in validation dataset (if implemented) every N steps. If this is set</span>
<span class="sd">                to `None` (default option), evaluation will happen at the end of every epoch.</span>
<span class="sd">            checkpoint_every (`str`, *optional*, defaults to `epoch`):</span>
<span class="sd">                Checkpoint every N epochs, steps or evaluations. Requires a number and a unit in a string.</span>
<span class="sd">                The following examples are valid:</span>

<span class="sd">                - `&quot;epoch&quot;`, `&quot;ep&quot;`, `&quot;1epoch&quot;`, `&quot;1ep&quot;`, `&quot;1 epoch&quot;`, `&quot;1 ep&quot;`: 1 Epoch</span>
<span class="sd">                - `&quot;step&quot;`, `&quot;st&quot;`, `&quot;1step&quot;`, `&quot;1st&quot;`, `&quot;1 step&quot;`, `&quot;1 st&quot;`: 1 Step</span>
<span class="sd">                - `&quot;evaluation&quot;`, `&quot;eval&quot;`, `&quot;1evaluation&quot;`, `&quot;1eval&quot;`, `&quot;1 evaluation&quot;`, `&quot;1 eval&quot;`: 1 Evaluation</span>

<span class="sd">                (a character `s` at the end of the string is also valid)</span>

<span class="sd">                If set to `None`, checkpointing will be disabled.</span>
<span class="sd">            logging_dir (`str`, *optional*, defaults to `logs`):</span>
<span class="sd">                Path where to save logs to show progress. It can be an IP address (local or remote), HTTP or HTTPS link,</span>
<span class="sd">                or simply a directory.</span>
<span class="sd">            log_with (`str`, *optional*, defaults to `None`):</span>
<span class="sd">                Logger to log metrics. It can be one of the following:</span>
<span class="sd">                    - `mlflow`</span>

<span class="sd">                NOTE: MLFlow is the only one supported right now. Other trackers are not currently available.</span>
<span class="sd">            log_every (`int`, *optional*, defaults to `-1`):</span>
<span class="sd">                Log train loss every N steps. If set to `-1`, training loss will be logged at the end of every epoch (or if gradient accumulation</span>
<span class="sd">                is enabled, the value will be the length of the training dataloader divided by the number of accumulation steps).</span>
<span class="sd">                If gradient accumulation is enabled and the value is not `-1`, this value will be multiplied by the number of accumulation steps.</span>
<span class="sd">            grad_accumulation_steps (`int`, *optional*, defaults to `None`):</span>
<span class="sd">                Accumulate gradients for N steps. Useful for training large models and simulate</span>
<span class="sd">                large batches when memory is not enough. If set to `None` or `1`, no accumulation will be perfomed.</span>
<span class="sd">            gradient_checkpointing (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Use gradient checkpointing. It requires a `gradient_checkpointing_enable` method in the model (models from</span>
<span class="sd">                HuggingFace&#39;s `transformers` library have this method already implemented) with a single argument `gradient_checkpointing_kwargs`</span>
<span class="sd">                (can be a dictionary or `None`).</span>
<span class="sd">            gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):</span>
<span class="sd">                Keyword arguments for `gradient_checkpointing_enable` method.</span>
<span class="sd">            clip_grad (`float`, *optional*, defaults to 1.0):</span>
<span class="sd">                Performs gradient clipping in between backpropagation and optimizer&#39;s step function.</span>
<span class="sd">            set_to_none (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                From PyTorch documentation: &quot;instead of setting to zero, set the grads to None. This will</span>
<span class="sd">                in general have lower memory footprint, and can modestly improve performance.&quot; Some</span>
<span class="sd">                optimizers have a different behaviour if the gradient is 0 or None. See PyTorch docs</span>
<span class="sd">                for more information: https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html</span>
<span class="sd">            shuffle_train (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to shuffle train DataLoader.</span>
<span class="sd">            sampler (`list` or `Any`, *optional*, defaults to `None`):</span>
<span class="sd">                Sampler (or list of samplers) for train DataLoader.</span>
<span class="sd">            collate_fn (`Callable`, *optional*, defaults to `None`):</span>
<span class="sd">                Collate function to be implemented in both train and validation dataloaders.</span>
<span class="sd">            collate_fn_train (`Callable`, *optional*, defaults to `None`):</span>
<span class="sd">                Collate function to be implemented in train dataloader. Cannot be imlpemented if `collate_fn` was</span>
<span class="sd">                already declared.</span>
<span class="sd">            collate_fn_val (`Callable`, *optional*, defaults to `None`):</span>
<span class="sd">                Collate function to be implemented in validation dataloader. Cannot be implemented if `collate_fn` was</span>
<span class="sd">                already declared.</span>
<span class="sd">            max_shard_size (`str`, *optional*, defaults to `10GB`):</span>
<span class="sd">                Max model shard size to be used.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to save model using safe tensors or the traditional PyTorch way. If `True`, some tensors</span>
<span class="sd">                will be lost.</span>
<span class="sd">            compile (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to call `torch.compile` on model (and teacher, if implemented).</span>
<span class="sd">            compile_kwargs (`dict`, *optional*, defaults to `None`):</span>
<span class="sd">                `torch.compile` kwargs for additional customization.</span>
<span class="sd">            safe_mode (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Run forward passes of the model in safe mode. This means that the forward pass of the model will run</span>
<span class="sd">                through the corresponding wrapper (DDP, FSDP or DeepSpeedEngine). If not running in safe mode, forward pass</span>
<span class="sd">                will skip the wrapper and run directly on the module (instance of `nn.Module`). Running with safe mode disabled</span>
<span class="sd">                will slightly improve throughput, although gradients consistency and mixed precision could be affected because</span>
<span class="sd">                skipping the wrapper&#39;s forward pass might skip internal parallel functionality.</span>

<span class="sd">                **NOTE**: This parameter takes no effect running with FSDP since forward passes are already done through this wrapper.</span>
<span class="sd">            train_loss_metric_name (`str`, *optional*, defaults to `train_loss`):</span>
<span class="sd">                Metric name for train loss in logs.</span>
<span class="sd">            val_loss_metric_name (`str`, *optional*, defaults to `val_loss`):</span>
<span class="sd">                Metric name for validation loss in logs.</span>
<span class="sd">            dataloader_pin_memory (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Enables pin memory option in DataLoader (only if GPU is enabled).</span>
<span class="sd">            dataloader_num_workers (`int`, *optional*, defaults to `None`):</span>
<span class="sd">                Number of processes for DataLoader. This defaults to `None`, meaning the number of workers will be equal to the</span>
<span class="sd">                number of processes set for training.</span>
<span class="sd">            dataloader_drop_last (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to drop last batch on DataLoader or not.</span>
<span class="sd">            eval_when_finish (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                At the end of training, evaluate model on validation dataset (if available). This option is only valid when</span>
<span class="sd">                `evaluate_every_n_steps` is not `None`.</span>
<span class="sd">            eval_when_start (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Start training with evaluation (if available).</span>
<span class="sd">            monitor (`Monitor` or `dict`, *optional*, defaults to `None`):</span>
<span class="sd">                Monitor arguments to keep track of variables during training. If not specified, &#39;train_loss&#39; and &#39;validation_loss&#39; will</span>
<span class="sd">                be set to `True` by default.</span>

<span class="sd">                NOTE: Learning rate, GPU and CPU monitoring will only be reported during training, not evaluation. Also, GPU and CPU</span>
<span class="sd">                monitoring will only be reported on main process (index 0).</span>
<span class="sd">            metrics (`Metric`, `list` or `dict`, *optional*, defaults to `None`):</span>
<span class="sd">                List of additional metrics of type &#39;Metric&#39; to track. When doing multiple evaluations, this should be a dictionary</span>
<span class="sd">                of metrics (or list of metrics), where each key corresponds to the dataset to evaluate (specified in `val_dataset`</span>
<span class="sd">                in `fit` function) and the value corresponds to a `Metric` or list of metrics. If metrics are given as only `Metric`</span>
<span class="sd">                or list of metrics, these metrics will apply for all evaluations. If you want specific metrics for specific evaluations,</span>
<span class="sd">                consider dividing your metrics per validation dataset in a dictionary.</span>
<span class="sd">            cleanup_cache_every_n_steps (`int`, *optional*, defaults to `None`):</span>
<span class="sd">                Cleanup CPU and CUDA caches every N steps. Default is no cleanup.</span>

<span class="sd">                NOTE: On every epoch and evaluation call we cleanup cache.</span>
<span class="sd">            callback (`Callback` or `list`, *optional*, defaults to `None`):</span>
<span class="sd">                `Callback` or callbacks to implement.</span>
<span class="sd">            additional_tracker_config (`dict`, *optional*, defaults to `None`):</span>
<span class="sd">                Additional configuration specification for tracker (e.g. hyper-parameters).</span>
<span class="sd">            batch_device_placement (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Move batches to correct device automatically. If `False`, batches will be in CPU.</span>
<span class="sd">            prepare_batch (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Prepares a batch dynamically when using Mixed Precision. When using DeepSpeed, we need to scale down</span>
<span class="sd">                the floating point tensors to be able to do calculations with the model. If not using DeepSpeed,</span>
<span class="sd">                this argument takes no effect.</span>
<span class="sd">            safe_steps (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Run safe training and validation steps to avoid OOMs (Out Of Memory errors) and retry steps. If a retry does not</span>
<span class="sd">                solve the problem, a list of users using GPUs will pop up and the OOM error will raise.</span>
<span class="sd">            destroy_after_training (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Destroy the process group after training. Set to `False` if you&#39;re running multiple trainings in the same script.</span>
<span class="sd">            enable_prepare_logging (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Enable internal model preparation logging. When using DeepSpeed, there are many messages that appear</span>
<span class="sd">                in the terminal that can be annoying.</span>
<span class="sd">            kwargs (`Any`, *optional*):</span>
<span class="sd">                Extra arguments for specific `init` function in Tracker, e.g. `run_name`, `tags`, etc.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># do some previous checks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_with</span> <span class="o">=</span> <span class="n">log_with</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">log_with</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">log_with</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span> <span class="o">=</span> <span class="n">_tracker_map</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">log_with</span><span class="p">]()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">DEBUG_MODE</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hps_config</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">HyperParameters</span><span class="p">)),</span> <span class="p">(</span>
            <span class="s2">&quot;&#39;hps_config&#39; needs to be either a string, dictionary or HyperParameters class.&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">clip_grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">clip_grad</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span> <span class="s2">&quot;&#39;clip_grad&#39; argument needs to be a float.&quot;</span>

        <span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">IS_GPU</span><span class="p">,</span> <span class="n">accelerator</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_gpu</span> <span class="o">=</span> <span class="n">IS_GPU</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">=</span> <span class="n">accelerator</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hps</span> <span class="o">=</span> <span class="n">HyperParameters</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">hps_config</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hps_config</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span> <span class="k">else</span> <span class="n">hps_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">track_name</span> <span class="o">=</span> <span class="n">track_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">CHECKPOINT_DIR</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_path</span> <span class="o">=</span> <span class="n">model_path</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">resume</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">multiple_checkpoints</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot specify a checkpoint index in &#39;resume&#39; when &#39;multiple_checkpoints&#39; is disabled.&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">resume</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">resume</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Checkpoint index in &#39;resume&#39; must be greater than 0 (or -1 to resume from latest checkpoint).&quot;</span>
                <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resume</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span>
                <span class="n">resume</span>
                <span class="k">if</span> <span class="n">resume</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">DEBUG_MODE</span> <span class="o">&lt;</span> <span class="mi">3</span>
            <span class="k">else</span> <span class="kc">False</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="n">metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disable_model_saving</span> <span class="o">=</span> <span class="n">disable_model_saving</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span>
            <span class="nb">str</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># key: model saving, value: (saving_below, saving_above)</span>

        <span class="k">if</span> <span class="n">patience</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">patience</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span> <span class="k">if</span> <span class="n">patience</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The &#39;patience&#39; argument in Trainer should have a value greater than 0.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">patience</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">patience</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">v</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;The &#39;patience&#39; argument when declared as a dictionary needs to have values above 0. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2"> in &#39;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">&#39;.&quot;</span>
                    <span class="p">)</span>
        <span class="k">elif</span> <span class="n">patience</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;patience&#39; must be either an integer value or a dictionary.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_every_n_steps</span> <span class="o">=</span> <span class="n">evaluate_every_n_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_checkpointing</span> <span class="o">=</span> <span class="n">enable_checkpointing</span> <span class="k">if</span> <span class="n">DEBUG_MODE</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="k">else</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multiple_checkpoints</span> <span class="o">=</span> <span class="n">multiple_checkpoints</span>
        <span class="k">if</span> <span class="n">max_checkpoints</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">max_checkpoints</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;max_checkpoints&#39; must be greater than 0 or `None`.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_checkpoints</span> <span class="o">=</span> <span class="n">max_checkpoints</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_every</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_strat</span> <span class="o">=</span> <span class="n">get_number_and_unit</span><span class="p">(</span><span class="n">checkpoint_every</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logging_dir</span> <span class="o">=</span> <span class="n">logging_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_every</span> <span class="o">=</span> <span class="n">log_every</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation_steps</span> <span class="o">=</span> <span class="n">grad_accumulation_steps</span> <span class="k">if</span> <span class="n">grad_accumulation_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="n">gradient_checkpointing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing_kwargs</span> <span class="o">=</span> <span class="n">gradient_checkpointing_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad</span> <span class="o">=</span> <span class="n">clip_grad</span> <span class="k">if</span> <span class="n">clip_grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mf">0.0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">==</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">DEEPSPEED</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">deepspeed_plugin</span><span class="o">.</span><span class="n">deepspeed_config</span><span class="p">[</span><span class="s2">&quot;gradient_clipping&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_to_none</span> <span class="o">=</span> <span class="n">set_to_none</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shuffle_train</span> <span class="o">=</span> <span class="n">shuffle_train</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">sampler</span>
        <span class="k">if</span> <span class="n">collate_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">collate_fn_train</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">collate_fn_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;collate_fn&#39; cannot be declared along with &#39;collate_fn_train&#39; or &#39;collate_fn_val&#39;.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span> <span class="o">=</span> <span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn_train</span> <span class="o">=</span> <span class="n">collate_fn_train</span> <span class="k">if</span> <span class="n">collate_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn_val</span> <span class="o">=</span> <span class="n">collate_fn_val</span> <span class="k">if</span> <span class="n">collate_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_shard_size</span> <span class="o">=</span> <span class="n">max_shard_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">safe_serialization</span> <span class="o">=</span> <span class="n">safe_serialization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compile</span> <span class="o">=</span> <span class="nb">compile</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compile_kwargs</span> <span class="o">=</span> <span class="n">compile_kwargs</span> <span class="k">if</span> <span class="n">compile_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">safe_mode</span> <span class="o">=</span> <span class="n">safe_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_loss_metric_name</span> <span class="o">=</span> <span class="n">train_loss_metric_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_metric_name</span> <span class="o">=</span> <span class="n">val_loss_metric_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataloader_pin_memory</span> <span class="o">=</span> <span class="n">dataloader_pin_memory</span> <span class="k">if</span> <span class="n">IS_GPU</span> <span class="k">else</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataloader_num_workers</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">dataloader_num_workers</span> <span class="k">if</span> <span class="n">dataloader_num_workers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">num_processes</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">DEBUG_MODE</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader_num_workers</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># force when debugging to not have problems with dataloader during breakpoints</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataloader_num_workers</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataloader_drop_last</span> <span class="o">=</span> <span class="n">dataloader_drop_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">samplers</span> <span class="o">=</span> <span class="n">sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_when_finish</span> <span class="o">=</span> <span class="n">eval_when_finish</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_when_start</span> <span class="o">=</span> <span class="n">eval_when_start</span> <span class="k">if</span> <span class="n">DEBUG_MODE</span> <span class="o">&lt;</span> <span class="mi">4</span> <span class="k">else</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span> <span class="o">=</span> <span class="n">monitor</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">monitor</span><span class="p">,</span> <span class="n">Monitor</span><span class="p">)</span> <span class="k">else</span> <span class="n">Monitor</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">monitor</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cleanup_cache_every_n_steps</span> <span class="o">=</span> <span class="n">cleanup_cache_every_n_steps</span>
        <span class="n">callback</span> <span class="o">=</span> <span class="n">callback</span> <span class="k">if</span> <span class="n">callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">Callback</span><span class="p">()</span>
        <span class="n">callback</span> <span class="o">=</span> <span class="n">callback</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callback</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="p">[</span><span class="n">callback</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span> <span class="o">=</span> <span class="n">CallbackMaster</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">additional_tracker_config</span> <span class="o">=</span> <span class="n">additional_tracker_config</span> <span class="k">if</span> <span class="n">additional_tracker_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_device_placement</span> <span class="o">=</span> <span class="n">batch_device_placement</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_batch</span> <span class="o">=</span> <span class="n">prepare_batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">safe_steps</span> <span class="o">=</span> <span class="n">safe_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">destroy_after_training</span> <span class="o">=</span> <span class="n">destroy_after_training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_prepare_logging</span> <span class="o">=</span> <span class="n">enable_prepare_logging</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">project_configuration</span> <span class="o">=</span> <span class="n">ProjectConfiguration</span><span class="p">(</span>
            <span class="n">project_dir</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">logging_dir</span><span class="o">=</span><span class="n">logging_dir</span><span class="p">,</span> <span class="n">total_limit</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_logging</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">TrainingState</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gatherer</span> <span class="o">=</span> <span class="n">Gatherer</span><span class="p">()</span>
        <span class="c1"># adding a total (at maximum) of 64 bytes for additional tensors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_loss_state</span> <span class="o">=</span> <span class="n">LossState</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_every</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="n">IS_GPU</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">LossState</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># prepare val loss states in &#39;fit&#39; function</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_checkpointing_every_n_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_strat</span> <span class="o">==</span> <span class="s2">&quot;step&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_checkpointing_after_evaluation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_strat</span> <span class="o">==</span> <span class="s2">&quot;eval&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_checkpointing_when_epoch_ends</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_strat</span> <span class="o">==</span> <span class="s2">&quot;epoch&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_module</span><span class="p">:</span> <span class="n">AcceleratorModule</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span><span class="p">:</span> <span class="n">LRScheduler</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_evaluations</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unwrapped_model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wrapped_model</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span> <span class="o">=</span> <span class="n">AsyncState</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">)</span> <span class="k">if</span> <span class="n">ASYNC</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">async_queue</span> <span class="o">=</span> <span class="n">AsyncDiskQueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="p">)</span> <span class="k">if</span> <span class="n">ASYNC</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tunnel</span> <span class="o">=</span> <span class="n">ModelTunnel</span><span class="p">(</span><span class="n">ASYNC_HASH</span><span class="p">)</span> <span class="k">if</span> <span class="n">ASYNC</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># initialize trackers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_id</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tracker_initialized</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logging</span> <span class="ow">and</span> <span class="n">DEBUG_MODE</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="p">((</span><span class="n">ASYNC</span> <span class="ow">and</span> <span class="n">ASYNC_TRAIN_GROUP</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">ASYNC</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">run_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_trackers</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tracker_initialized</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_model_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span></div>


<div class="viewcode-block" id="Trainer.fit">
<a class="viewcode-back" href="../../api.html#accmt.Trainer.fit">[docs]</a>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AcceleratorModule</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]],</span>
        <span class="n">train_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dataset</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">val_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Dataset</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Dataset</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to train a given `AcceleratorModule`.</span>

<span class="sd">        Args:</span>
<span class="sd">            module (`AcceleratorModule`, `str` or `tuple`):</span>
<span class="sd">                `AcceleratorModule` class containig the training logic. This can also be a string specifying a</span>
<span class="sd">                HuggingFace model, or a tuple of type (model, type), where &#39;model&#39; is a string for the HuggingFace model,</span>
<span class="sd">                and &#39;type&#39; is a string or class (from transformers library) for the model type.</span>
<span class="sd">            train_dataset (`torch.utils.data.Dataset`, *optional*, defaults to `None`):</span>
<span class="sd">                `Dataset` class from PyTorch containing the train dataset logic. If not provided, then</span>
<span class="sd">                `get_train_dataloader` from `module` will be used to get the train DataLoader.</span>
<span class="sd">            val_dataset (`torch.utils.data.Dataset`, `list` or `dict`, *optional*, defaults to `None`):</span>
<span class="sd">                `Dataset` class from PyTorch containing the validation dataset logic. This can also be a list or a dictionary</span>
<span class="sd">                of `Dataset`, in that case, multiple evaluations will run following the logic of `validation_step` and</span>
<span class="sd">                specified metrics. Metric names reported for a multiple evaluation setting will add a &#39;_&#39; followed by a key</span>
<span class="sd">                related to the dataset (e.g. &#39;accuracy_1&#39; or &#39;accuracy_another_dataset&#39;).</span>

<span class="sd">                If this dataset is not specified, then the validation logic of `AcceleratorModule`</span>
<span class="sd">                (if specified) will be skipped.</span>
<span class="sd">            kwargs (`Any`):</span>
<span class="sd">                Keyword arguments for `from_pretrained` function for model initialization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># reset loss states in case of another fit function call in the script</span>
        <span class="n">cleanup</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_loss_state</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_state</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">v</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_module</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">module</span><span class="o">.</span><span class="n">log_every</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_every</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unwrapped_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;`AcceleratorModule` subclass requires `self.model` and needs to be an instance of `nn.Module`.&quot;</span>
            <span class="p">)</span>

        <span class="n">teacher</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">teacher</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">teacher</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">teacher</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="n">teacher</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">module</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span>
        <span class="n">module</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span>
        <span class="n">module</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">device</span>

        <span class="k">if</span> <span class="n">MASTER_PROCESS</span> <span class="ow">and</span> <span class="n">DEBUG_MODE</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">enable_checkpointing</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_model_saving</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_dataset</span> <span class="k">if</span> <span class="n">val_dataset</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span> <span class="k">else</span> <span class="p">[</span><span class="n">val_dataset</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_evaluations</span> <span class="o">=</span> <span class="n">val_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dataloaders</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_metrics</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">val_dataloader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">[</span><span class="s2">&quot;best_valid_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">[</span><span class="s2">&quot;best_train_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">patience_left</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Keys declared in &#39;patience&#39; do not match model savings.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">patience_left</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">m</span><span class="o">.</span><span class="n">main_metric</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">v</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">val_dataloader</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
            <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_current_checkpoint_path</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">checkpoint_path</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;checkpoint_0&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="s2">&quot;Checkpoint directory is empty or not found.&quot;</span><span class="p">)</span>
            <span class="n">training_state_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">STATE_FILE</span><span class="p">)</span>
            <span class="n">loss_tracker_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">TRAIN_LOSS_STATE_FILE</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">training_state_path</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_loss_state</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">loss_tracker_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Training process has been flagged as finished.&quot;</span><span class="p">)</span>

        <span class="n">module</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">_set_extra</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loss_metric_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_metric_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">_tracking</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">==</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">FSDP</span><span class="p">:</span>
            <span class="c1"># preparing model before dataloaders is only supported by FSDP apparently, and this is the</span>
            <span class="c1"># recommended setting to prepare training.</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">LossState</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">include_per_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_gpu</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">val_dataloader</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">}</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_optimizer</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">step_scheduler_per_epoch</span><span class="p">:</span>
            <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_scheduler</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">epochs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">max_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_training_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">max_steps</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_training_steps</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">num_processes</span><span class="p">))</span>
            <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_scheduler</span><span class="p">(</span>
                <span class="n">module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">,</span> <span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># ignore epochs to avoid multiplication</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_scheduler</span><span class="p">(</span>
                <span class="n">module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">num_processes</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">epochs</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">ASYNC</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">ASYNC_TRAIN_GROUP</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tunnel</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tunnel_ready</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">run_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">run_id</span><span class="p">)</span>
                <span class="c1"># only MASTER_PROCESS returns a valid &#39;run_id&#39;, and &#39;update&#39; function already handles that.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">wait_for_tunnel</span><span class="p">()</span>

        <span class="n">model</span><span class="p">,</span> <span class="n">teacher</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">teacher</span><span class="p">,</span>
            <span class="n">train_dataloader</span><span class="p">,</span>
            <span class="n">val_dataloader</span><span class="p">,</span>
            <span class="n">optimizer</span><span class="p">,</span>
            <span class="n">scheduler</span><span class="p">,</span>
            <span class="n">batch_device_placement</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_device_placement</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_model_dtype</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">dtype</span>

        <span class="k">if</span> <span class="n">ASYNC</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">ASYNC_TRAIN_GROUP</span><span class="p">:</span>
            <span class="c1"># force train dataloader, optimizer and scheduler to be None in evaluation group since they&#39;re not being used.</span>
            <span class="n">train_dataloader</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">scheduler</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="n">scheduler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="n">module</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler</span>
        <span class="n">module</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wrapped_model</span> <span class="o">=</span> <span class="n">model</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_every</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># report training loss at the last step (or end of an epoch)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_every</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation_steps</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation_steps</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_every</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation_steps</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_every</span>

        <span class="k">for</span> <span class="n">callback</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">children</span><span class="p">:</span>
            <span class="n">callback</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
            <span class="n">callback</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="bp">self</span>
            <span class="n">callback</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_fit_start</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">ASYNC</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">ASYNC_TRAIN_GROUP</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dispatch_async_eval</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loop</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">ASYNC</span> <span class="ow">and</span> <span class="n">ASYNC_TRAIN_GROUP</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">train_finished</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># wait until evaluation group is finished</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">evaluation_finished</span><span class="p">:</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

            <span class="c1"># evaluation group delegates the job to train group</span>
            <span class="n">eval_runs_pending</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">evaluations_in_queue</span>
            <span class="k">if</span> <span class="n">eval_runs_pending</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">eval_runs_pending</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_async_eval</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">tunnel</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">finished</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_fit_end</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">free_memory</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">get_tracker</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_with</span><span class="p">)</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">destroy_after_training</span> <span class="ow">and</span> <span class="n">WORLD_SIZE</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># done to avoid pytorch distributed warnings if script finishes here</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unwrapped_model</span></div>

            <span class="c1"># TODO still getting memory leaks if running multiple trainings using the very same module</span>

    <span class="k">def</span> <span class="nf">loop</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">train_dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
        <span class="n">val_dataloader</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">]],</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">scheduler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LRScheduler</span><span class="p">],</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs a training loop.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">evaluations_done</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_when_start</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">launch_eval</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_iterator</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_iterator</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_train_logic</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span>

                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_every_n_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_every_n_steps</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">launch_eval</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_every_n_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_when_finish</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_last_epoch</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">launch_eval</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">dispatch_async_eval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">],</span> <span class="n">delay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="p">):</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">train_finished</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_async_eval</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>

            <span class="c1"># continue checking for evaluations</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">delay</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">evaluation_finished</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_async_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">]):</span>
        <span class="n">evals_in_queue</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">evaluations_in_queue</span>
        <span class="k">if</span> <span class="n">evals_in_queue</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># read last model from SHM</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tunnel</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">evals_in_queue</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="c1"># read next model from disk and write it into SHM</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_queue</span><span class="o">.</span><span class="n">dequeue</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tunnel</span><span class="o">.</span><span class="n">write_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">evaluations_in_queue</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">launch_eval</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">dataloader</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">ASYNC</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">wait_for_everyone</span><span class="p">()</span>
            <span class="n">unwrapped_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">unwrap_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">evaluations_in_queue</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># SHM is free</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tunnel</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># SHM waiting, then we write to disk</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">async_queue</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">evaluations_in_queue</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">wait_for_everyone</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>

        <span class="n">should_save_model</span> <span class="o">=</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_when_start</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">evaluations_done</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># not doing first requested evaluation</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checkpointing_after_evaluation</span> <span class="ow">and</span> <span class="n">should_save_model</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_checkpoint</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_end_of_epoch</span> <span class="k">else</span> <span class="mi">1</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">train_step</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_end_of_epoch</span> <span class="k">else</span> <span class="mi">0</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_end_of_epoch</span> <span class="k">else</span> <span class="mi">0</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">evaluations_done</span><span class="p">,</span>
                <span class="n">finished</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">finished</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">]]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs evaluation on a given dataloader.&quot;&quot;&quot;</span>
        <span class="n">no_patience_left</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">v</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">patience_left</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">DEBUG_MODE</span> <span class="o">&gt;=</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">no_patience_left</span> <span class="ow">or</span> <span class="n">dataloader</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="n">cleanup</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_evaluation_start</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">val_dataloader</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">val_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot; (</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">) &quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_evaluations</span> <span class="k">else</span> <span class="s2">&quot; &quot;</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
                <span class="n">iterable</span><span class="o">=</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">),</span>
                <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">),</span>
                <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">val_str</span><span class="si">}</span><span class="s2">Evaluating in Epoch </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">epochs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">colour</span><span class="o">=</span><span class="s2">&quot;cyan&quot;</span><span class="p">,</span>
                <span class="o">**</span><span class="n">_tqdm_kwargs</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">val_step</span> <span class="o">=</span> <span class="n">i</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_last_validation_batch</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_batch</span> <span class="k">else</span> <span class="n">batch</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_validation_logic</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="s2">&quot;valid_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_state</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">get_total_loss</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">metric</span><span class="o">.</span><span class="n">_parallel</span> <span class="ow">and</span> <span class="n">MASTER_PROCESS</span><span class="p">:</span>
                        <span class="c1"># we don&#39;t want to call &#39;_compute&#39; for metrics that are implemented in main process,</span>
                        <span class="c1"># since the state on other processes is empty</span>
                        <span class="n">metric_dict</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">_compute</span><span class="p">()</span>

                        <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">metric_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
                                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                    <span class="sa">f</span><span class="s2">&quot;Value in metric&#39;s dict does not accept </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="si">}</span><span class="s2">, only &quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot;`float`, `int`, `torch.Tensor` (torch) or `NDArray` (numpy)&quot;</span>
                                <span class="p">)</span>

                            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="n">v</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">))</span> <span class="k">else</span> <span class="n">v</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                            <span class="p">)</span>
                    <span class="k">elif</span> <span class="n">metric</span><span class="o">.</span><span class="n">_parallel</span><span class="p">:</span>
                        <span class="n">metric_dict</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">_compute</span><span class="p">()</span>
                        <span class="c1"># we are not fixing objects since in parallel mode they&#39;re already converted to python values</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">metric_dict</span><span class="p">)</span>

            <span class="c1"># re-format metrics, instead of a dict dataset_key (key) and metrics (dictionary value), gather</span>
            <span class="c1"># all metrics into a single dictionary with the format {metric__dataset_key: value}.</span>
            <span class="c1"># e.g. {&quot;accuracy__dataset1&quot;: 0.21, &quot;accuracy__dataset2&quot;: 0.67}</span>
            <span class="n">log_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">_metric_name</span><span class="p">,</span> <span class="n">_value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">_metric_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;best_&quot;</span><span class="p">):</span>
                    <span class="k">continue</span>
                <span class="n">_metric_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">_metric_name</span><span class="si">}</span><span class="s2">__</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiple_evaluations</span> <span class="k">else</span> <span class="n">_metric_name</span>
                <span class="n">log_dict</span><span class="p">[</span><span class="n">_metric_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">_value</span>

            <span class="n">run_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_state</span><span class="o">.</span><span class="n">run_id</span> <span class="k">if</span> <span class="n">ASYNC</span> <span class="ow">and</span> <span class="n">MASTER_PROCESS</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">log_additional_metrics</span><span class="p">(</span><span class="n">log_dict</span><span class="p">,</span> <span class="n">run_id</span><span class="o">=</span><span class="n">run_id</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">evaluations_done</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">should_save_model</span> <span class="o">=</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_when_start</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">evaluations_done</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># not doing first requested evaluation</span>

        <span class="c1"># save model</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">should_save_model</span> <span class="ow">and</span> <span class="n">DEBUG_MODE</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_model_on_criteria</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># reset total loss state for validation since it&#39;s not being used</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_state</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_state</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">total_loss</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_state</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">num_steps</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">val_step</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># flag as finished if doing very last evaluation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">finished</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_last_training_batch</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_last_epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_evaluation_end</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_save_model_on_criteria</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save model depending on criteria defined in `model_saving`&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">wait_for_everyone</span><span class="p">()</span>

        <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loss_state</span><span class="o">.</span><span class="n">get_total_loss</span><span class="p">()</span>
        <span class="n">can_save</span> <span class="o">=</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_when_start</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">evaluations_done</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_check_and_save</span><span class="p">(</span><span class="n">model_saving</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">_model_saving</span> <span class="o">=</span> <span class="n">model_saving</span>
            <span class="n">model_saving_without_prefix</span> <span class="o">=</span> <span class="n">model_saving</span><span class="o">.</span><span class="n">removeprefix</span><span class="p">(</span><span class="s2">&quot;best_&quot;</span><span class="p">)</span>
            <span class="c1"># we already have all metrics calculated per dataset in self.state.additional_metrics</span>
            <span class="n">metrics_and_datasets</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span>
                <span class="nb">list</span>
            <span class="p">)</span>  <span class="c1"># e.g. {&quot;accuracy&quot;: [&quot;dataset1&quot;, &quot;dataset2&quot;], &quot;metric&quot;: [dataset_keys, ...]}</span>
            <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">model_saving_without_prefix</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">):</span>
                <span class="n">metric</span><span class="p">,</span> <span class="o">*</span><span class="n">datasets</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;@&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">datasets</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># if datasets are not specified for a metric, then it means that we need to average</span>
                    <span class="c1"># across all datasets</span>
                    <span class="n">datasets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

                <span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="n">datasets</span><span class="p">:</span>
                    <span class="n">metrics_and_datasets</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

            <span class="c1"># now create a buffer per metric, where each value in the buffer corresponds to the</span>
            <span class="c1"># metric found in a dataset</span>
            <span class="n">metric_buffer</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>  <span class="c1"># e.g. {&quot;accuracy&quot;: [0.2, 0.5], &quot;metric&quot;: [values, ...]}</span>
            <span class="k">for</span> <span class="n">dataset_key</span><span class="p">,</span> <span class="n">metrics_dict</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">dataset_key</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">metrics_and_datasets</span><span class="p">[</span><span class="n">metric</span><span class="p">]):</span>
                        <span class="n">metric_buffer</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

            <span class="c1"># now average those metrics in buffer</span>
            <span class="n">metric_avgs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">metric_buffer</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

            <span class="n">_metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;@&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">ms</span> <span class="ow">in</span> <span class="n">model_saving_without_prefix</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)]</span>
            <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">_metrics</span><span class="p">:</span>
                <span class="n">best_metric_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;best_</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="n">comparator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_comparator</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span> <span class="k">if</span> <span class="n">metric</span> <span class="o">!=</span> <span class="s2">&quot;valid_loss&quot;</span> <span class="k">else</span> <span class="s2">&quot;&lt;&quot;</span>
                <span class="n">compare</span> <span class="o">=</span> <span class="n">operator_map</span><span class="p">[</span><span class="n">comparator</span><span class="p">]</span>
                <span class="n">new</span> <span class="o">=</span> <span class="n">metric_avgs</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span>
                <span class="c1"># calculate average between previous metrics in wanted datasets</span>
                <span class="n">prev</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">dataset_key</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">metrics_and_datasets</span><span class="p">[</span><span class="n">metric</span><span class="p">]):</span>
                    <span class="k">if</span> <span class="n">best_metric_str</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">dataset_key</span><span class="p">]:</span>
                        <span class="c1"># only register best metrics in wanted datasets</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">dataset_key</span><span class="p">][</span><span class="n">best_metric_str</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">comparator</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;&lt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;=&quot;</span><span class="p">,</span> <span class="s2">&quot;==&quot;</span><span class="p">}</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)</span>
                        <span class="p">)</span>

                    <span class="n">prev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">dataset_key</span><span class="p">][</span><span class="n">best_metric_str</span><span class="p">])</span>

                <span class="n">prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">prev</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prev</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">prev</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">saving_below</span><span class="p">,</span> <span class="n">saving_above</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">[</span><span class="n">_model_saving</span><span class="p">]</span>
                <span class="n">is_better</span> <span class="o">=</span> <span class="n">compare</span><span class="p">(</span><span class="n">new</span><span class="p">,</span> <span class="n">prev</span><span class="p">)</span> <span class="ow">and</span> <span class="n">new</span> <span class="o">&lt;</span> <span class="n">saving_below</span> <span class="ow">and</span> <span class="n">new</span> <span class="o">&gt;</span> <span class="n">saving_above</span>
                <span class="k">if</span> <span class="n">is_better</span><span class="p">:</span>
                    <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># register best metrics for all wanted datasets</span>
                <span class="k">for</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">new_metric_calculated</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">metrics_and_datasets</span><span class="p">[</span><span class="n">metric</span><span class="p">],</span> <span class="n">metric_buffer</span><span class="p">[</span><span class="n">metric</span><span class="p">]):</span>
                    <span class="n">local_prev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">dataset</span><span class="p">][</span><span class="n">best_metric_str</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">compare</span><span class="p">(</span><span class="n">new_metric_calculated</span><span class="p">,</span> <span class="n">local_prev</span><span class="p">):</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">additional_metrics</span><span class="p">[</span><span class="n">dataset</span><span class="p">][</span><span class="n">best_metric_str</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_metric_calculated</span>

                <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">_metrics</span><span class="p">):</span>
                    <span class="c1"># all these metrics have improved</span>
                    <span class="k">if</span> <span class="n">MASTER_PROCESS</span> <span class="ow">and</span> <span class="n">can_save</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_model_saving</span><span class="p">:</span>
                        <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="n">_model_saving</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;__&quot;</span><span class="p">))</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_save_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_path</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">can_save</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">patience_left</span><span class="p">[</span><span class="n">_model_saving</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">patience_left</span><span class="p">[</span><span class="n">_model_saving</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_train_loss</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">best_train_loss</span> <span class="o">=</span> <span class="n">train_loss</span>
                <span class="k">if</span> <span class="s2">&quot;best_train_loss&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">MASTER_PROCESS</span> <span class="ow">and</span> <span class="n">can_save</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_model_saving</span><span class="p">:</span>
                        <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="s2">&quot;best_train_loss&quot;</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_save_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_path</span><span class="p">)</span>
            <span class="k">elif</span> <span class="p">(</span>
                <span class="n">can_save</span> <span class="ow">and</span> <span class="s2">&quot;best_train_loss&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">patience_left</span><span class="p">[</span><span class="s2">&quot;best_train_loss&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">patience_left</span><span class="p">[</span><span class="s2">&quot;best_train_loss&quot;</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>

            <span class="k">for</span> <span class="n">model_saving</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">:</span>  <span class="c1"># TODO we could implement a tqdm bar maybe...</span>
                <span class="n">_check_and_save</span><span class="p">(</span><span class="n">model_saving</span><span class="p">)</span>

            <span class="c1"># if all model savings have no patience anymore, finish training process</span>
            <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">model_saving</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">count</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">patience_left</span><span class="p">[</span><span class="n">model_saving</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">wait_for_everyone</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">):</span>
                <span class="n">rprint</span><span class="p">(</span><span class="s2">&quot;Ran out of patience. Process finished.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">finished</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">if</span> <span class="n">MASTER_PROCESS</span><span class="p">:</span>
                    <span class="n">state_in_checkpoint</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">STATE_FILE</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state_in_checkpoint</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">model_saving</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">:</span>
                        <span class="n">model_saving_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_saving</span><span class="p">)</span>
                        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">model_saving_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                        <span class="n">model_saving_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_saving_path</span><span class="p">,</span> <span class="n">STATE_FILE</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_saving_path</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">end_training</span><span class="p">()</span>
                <span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save model inside a path.&quot;&quot;&quot;</span>
        <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\r</span><span class="si">{</span><span class="n">time_prefix</span><span class="p">()</span><span class="si">}</span><span class="s2"> Saving model...&quot;</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">unwrapped_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">unwrap_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">unwrapped_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">compile</span> <span class="k">else</span> <span class="n">unwrapped_model</span><span class="o">.</span><span class="n">_orig_mod</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="p">,</span> <span class="s2">&quot;save_pretrained&quot;</span><span class="p">):</span>  <span class="c1"># special function for models from transformers library</span>
            <span class="n">unwrapped_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span>
                <span class="n">path</span><span class="p">,</span>
                <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
                <span class="n">max_shard_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_shard_size</span><span class="p">,</span>
                <span class="n">save_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">save</span><span class="p">,</span>
                <span class="n">safe_serialization</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pt_state_dict</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;pytorch_model.pt&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">pt_state_dict</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">safe_serialization</span><span class="p">)</span>

        <span class="n">training_state_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">STATE_FILE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">training_state_path</span><span class="p">)</span>

        <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[A</span><span class="se">\033</span><span class="s2">[K</span><span class="si">{</span><span class="n">time_prefix</span><span class="p">()</span><span class="si">}</span><span class="s2"> Model saved.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validation_logic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">,</span> <span class="n">dataloader_key</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs all the validation logic.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_before_validation_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">safe_steps</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_safe_step</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">validation_step</span><span class="p">,</span> <span class="n">dataloader_key</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">validation_step</span><span class="p">(</span><span class="n">dataloader_key</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="c1"># assume it&#39;s loss value, so convert wrap it into a dictionary</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_after_validation_step</span><span class="p">()</span>
        <span class="c1"># track loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val_loss_state</span><span class="p">[</span><span class="n">dataloader_key</span><span class="p">]</span><span class="o">.</span><span class="n">add_total_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="c1"># track metrics</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="n">dataloader_key</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">metric</span><span class="o">.</span><span class="n">main_metric</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Make sure to align &#39;validation_step&#39; with declared metrics.&quot;</span><span class="p">)</span>
                <span class="n">metric_compute_arguments</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="n">metric</span><span class="o">.</span><span class="n">main_metric</span><span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metric_compute_arguments</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                    <span class="n">metric_compute_arguments</span> <span class="o">=</span> <span class="p">(</span><span class="n">metric_compute_arguments</span><span class="p">,)</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="n">metric</span><span class="o">.</span><span class="n">_parallel</span><span class="p">:</span>
                    <span class="n">metric_compute_arguments</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="o">*</span><span class="p">(</span>
                            <span class="p">(</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">gatherer</span><span class="o">.</span><span class="n">all_gather_dictionary</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
                                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">gather_for_metrics</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
                            <span class="p">)</span>
                            <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">metric_compute_arguments</span>
                        <span class="p">),</span>  <span class="c1"># leave it as tuple</span>
                    <span class="p">)</span>

                    <span class="k">if</span> <span class="n">MASTER_PROCESS</span> <span class="ow">and</span> <span class="n">metric_compute_arguments</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">metric</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="o">*</span><span class="n">metric_compute_arguments</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">metric_compute_arguments</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">metric</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="o">*</span><span class="n">metric_compute_arguments</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare elements in a batch based on Mixed Precision. This function only takes effect when using DeepSpeed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">!=</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">DEEPSPEED</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">batch</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_nested_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_nested_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare nested batch. This function is derived from `transformers` library</span>
<span class="sd">        (https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">batch</span><span class="p">)({</span><span class="n">k</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_nested_batch</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">batch</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prepare_nested_batch</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">device</span><span class="p">}</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
                <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_dtype</span><span class="p">})</span>

            <span class="k">return</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">_safe_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;out of memory&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">wrapped_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">del</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">_e</span><span class="p">:</span>
                    <span class="n">rprint</span><span class="p">(</span><span class="s2">&quot;CUDA: Out Of Memory.&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="s2">&quot;out of memory&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">_e</span><span class="p">):</span>
                        <span class="n">print_gpu_users_by_device</span><span class="p">()</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">end</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="s2">&quot;FAILED&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">WORLD_SIZE</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>
                    <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">e</span>

    <span class="k">def</span> <span class="nf">_train_logic</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">scheduler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LRScheduler</span><span class="p">],</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs all the training logic.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">ASYNC</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">ASYNC_TRAIN_GROUP</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_before_training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation_steps</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">():</span>
            <span class="c1"># forward pass</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">safe_steps</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_safe_step</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">training_step</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_after_training_step</span><span class="p">()</span>

            <span class="c1"># track</span>
            <span class="n">_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_loss_state</span><span class="o">.</span><span class="n">add_batch_loss</span><span class="p">(</span><span class="n">_loss</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_loss_state</span><span class="o">.</span><span class="n">add_total_loss</span><span class="p">(</span><span class="n">_loss</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_before_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">module</span><span class="o">.</span><span class="n">_extended</span><span class="p">:</span>
                <span class="c1"># backpropagation</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_after_backward</span><span class="p">()</span>

            <span class="n">norm</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">sync_gradients</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">!=</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">DEEPSPEED</span>
            <span class="p">):</span>
                <span class="n">norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">batch_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loss_state</span><span class="o">.</span><span class="n">get_batch_loss</span><span class="p">()</span>

                <span class="k">if</span> <span class="n">MASTER_PROCESS</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">grad_norm</span> <span class="ow">and</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_grad_norm</span><span class="p">()</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">log_train_loss_and_grad_norm</span><span class="p">(</span><span class="n">batch_loss</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">module</span><span class="o">.</span><span class="n">_extended</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_before_optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_after_optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">step_scheduler_per_epoch</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_before_scheduler_step</span><span class="p">(</span><span class="n">scheduler</span><span class="p">)</span>
                    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_after_scheduler_step</span><span class="p">(</span><span class="n">scheduler</span><span class="p">)</span>

                <span class="c1"># reset gradients</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_before_optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">set_to_none</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_after_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">batch_iterator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Batch iterator for training handling checkpointing.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">model</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle_train</span><span class="p">:</span>
            <span class="n">global_seed</span> <span class="o">=</span> <span class="n">get_seed</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">set_seed</span><span class="p">(</span><span class="n">global_seed</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>
            <span class="n">dataloader</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">_dataloader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">skip_first_batches</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">train_step</span><span class="p">)</span>

        <span class="n">cleanup</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">train_step</span>

        <span class="c1"># determine total steps for the current epoch</span>
        <span class="n">total_steps_in_epoch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
        <span class="c1"># calculate remaining steps in current epoch</span>
        <span class="n">remaining_steps</span> <span class="o">=</span> <span class="n">total_steps_in_epoch</span> <span class="o">-</span> <span class="n">start</span>

        <span class="c1"># for progress bar, use max_steps if defined, otherwise use dataloader length</span>
        <span class="n">progress_total</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">max_steps</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">max_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">total_steps_in_epoch</span>
        <span class="n">progress_initial</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">max_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">start</span>

        <span class="k">if</span> <span class="n">remaining_steps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
                <span class="n">iterable</span><span class="o">=</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">_dataloader</span><span class="p">,</span> <span class="n">start</span><span class="p">),</span>
                <span class="n">total</span><span class="o">=</span><span class="n">progress_total</span><span class="p">,</span>
                <span class="n">initial</span><span class="o">=</span><span class="n">progress_initial</span><span class="p">,</span>
                <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot; Training in Epoch </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">epochs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">position</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">colour</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span>
                <span class="o">**</span><span class="n">_tqdm_kwargs</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">train_step</span> <span class="o">=</span> <span class="n">i</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_last_training_batch</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">total_steps_in_epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">max_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">max_steps</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">lr</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
                    <span class="p">)</span>

                    <span class="c1"># TODO we can fuse these functions to only report once to the server</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">log_learning_rate</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">log_cpu_utilization</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">log_gpu_utilization</span><span class="p">()</span>

                <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_batch</span> <span class="k">else</span> <span class="n">batch</span>
                <span class="k">yield</span> <span class="n">batch</span>

                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cleanup_cache_every_n_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">cleanup_cache_every_n_steps</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">):</span>
                    <span class="n">cleanup</span><span class="p">()</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checkpointing_every_n_steps</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_save_checkpoint</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">train_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">evaluations_done</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># check if we&#39;ve reached max_steps</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">max_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">max_steps</span><span class="p">:</span>
                    <span class="k">break</span>

        <span class="c1"># if length of _dataloader is 0, then we do not iterate</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_end_of_epoch</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">train_step</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">_save_checkpoint</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">train_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">global_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">evaluations_done</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">finished</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save checkpoint at a given point in time (`epoch` and `train_step`).&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_save_checkpoint</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">MASTER_PROCESS</span><span class="p">:</span>
            <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\r</span><span class="si">{</span><span class="n">time_prefix</span><span class="p">()</span><span class="si">}</span><span class="s2"> Saving checkpoint...&quot;</span><span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">wait_for_everyone</span><span class="p">()</span>
        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_path</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_checkpoints</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">MASTER_PROCESS</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_checkpoints</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">))</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_checkpoints</span>
            <span class="p">):</span>
                <span class="n">min_checkpoint</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
                <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">min_checkpoint</span><span class="p">))</span>

            <span class="n">last_checkpoint_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_current_checkpoint_path</span><span class="p">(</span><span class="n">ignore_resume_idx</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">new_checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;checkpoint_</span><span class="si">{</span><span class="n">last_checkpoint_num</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">MASTER_PROCESS</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">new_checkpoint_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">new_checkpoint_path</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">save_state</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">safe_serialization</span><span class="p">)</span>

        <span class="n">loss_tracker_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">TRAIN_LOSS_STATE_FILE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_loss_state</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">loss_tracker_path</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">num_checkpoints_made</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">MASTER_PROCESS</span><span class="p">:</span>
            <span class="n">training_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
            <span class="n">training_state_dict</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">epoch</span>
            <span class="n">training_state_dict</span><span class="p">[</span><span class="s2">&quot;train_step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_step</span>
            <span class="n">training_state_dict</span><span class="p">[</span><span class="s2">&quot;global_step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">global_step</span>
            <span class="n">training_state_dict</span><span class="p">[</span><span class="s2">&quot;evaluations_done&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">evaluations_done</span>
            <span class="n">training_state_dict</span><span class="p">[</span><span class="s2">&quot;finished&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">finished</span>

            <span class="n">training_state_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">STATE_FILE</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">training_state_path</span><span class="p">,</span> <span class="n">training_state_dict</span><span class="p">)</span>
            <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[A</span><span class="se">\033</span><span class="s2">[K</span><span class="si">{</span><span class="n">time_prefix</span><span class="p">()</span><span class="si">}</span><span class="s2"> Checkpoint saved.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">log_checkpoint</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">epoch_iterator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Epoch iterator handling logic for checkpointing.&quot;&quot;&quot;</span>
        <span class="n">start</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="n">epoch</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">log_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_end_of_epoch</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_last_epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">epochs</span> <span class="o">-</span> <span class="mi">1</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_epoch_start</span><span class="p">()</span>
            <span class="k">yield</span> <span class="n">epoch</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_epoch_end</span><span class="p">()</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module</span><span class="o">.</span><span class="n">_extended</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">step_scheduler_per_epoch</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_before_scheduler_step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_after_scheduler_step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checkpointing_when_epoch_ends</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_save_checkpoint</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">train_step</span><span class="p">,</span>  <span class="c1"># always 0 at this stage</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">evaluations_done</span><span class="p">,</span>
                    <span class="c1"># flag as finished if checkpointing at the end of the last epoch</span>
                    <span class="n">finished</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">is_last_epoch</span><span class="p">,</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">teacher</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">train_dataloader</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DataLoader</span><span class="p">],</span>
        <span class="n">val_dataloader</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">]],</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">],</span>
        <span class="n">scheduler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LRScheduler</span><span class="p">],</span>
        <span class="n">batch_device_placement</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DataLoader</span><span class="p">],</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LRScheduler</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Call Accelerate&#39;s backend to prepare instances for distributed training. This will also load states for objects</span>
<span class="sd">        in case of resuming training.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_prepare_logging</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">==</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">DEEPSPEED</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">deepspeed.utils</span> <span class="kn">import</span> <span class="n">logger</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;gradient_checkpointing_enable&quot;</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">==</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">DEEPSPEED</span><span class="p">:</span>
            <span class="c1"># DeepSpeed requires contiguous parameters</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">param</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">compile</span> <span class="ow">and</span> <span class="n">DEBUG_MODE</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">compile_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">teacher</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">teacher</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">teacher</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">compile_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">val_dataloader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataloader</span> <span class="ow">in</span> <span class="n">val_dataloader</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">val_dataloader</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare_data_loader</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">==</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">FSDP</span><span class="p">:</span>
            <span class="c1"># ignore model preparation since it was already done before (only in the case of FSDP)</span>
            <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">!=</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">DEEPSPEED</span> <span class="ow">and</span> <span class="n">teacher</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">teacher</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">teacher</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">safe_mode</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">==</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">FSDP</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">==</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">MULTI_GPU</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">_DistributedDataParallel</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">register_for_checkpointing</span><span class="p">(</span><span class="n">scheduler</span><span class="p">)</span>

        <span class="c1"># load states if resuming</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_resume</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="p">):</span>
                <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_current_checkpoint_path</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">checkpoint_path</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;checkpoint_0&quot;</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="s2">&quot;Checkpoint directory is empty or not found.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">load_state</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_path</span><span class="si">}</span><span class="s2">&#39; was not found.&quot;</span><span class="p">)</span>

        <span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_device_placement</span> <span class="ow">and</span> <span class="n">train_dataloader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">train_dataloader</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">cpu</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">val_dataloader</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">val_dataloader</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">cpu</span>

        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">teacher</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span>

    <span class="k">def</span> <span class="nf">_get_current_checkpoint_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ignore_resume_idx</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the checkpoint path based on the &#39;resume&#39; argument or the latest checkpoint.</span>
<span class="sd">        If this returns a path ending with &quot;checkpoint_0&quot;, it means that the checkpoint directory is empty or not found.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_path</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiple_checkpoints</span><span class="p">:</span>
            <span class="n">num_checkpoints</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">))</span> <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">num_checkpoints</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">resume</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">resume</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">ignore_resume_idx</span><span class="p">:</span>
                    <span class="c1"># load the checkpoint at the given index</span>
                    <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;checkpoint_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">resume</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># find the latest checkpoint by getting the maximum checkpoint number</span>
                    <span class="n">latest_checkpoint</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
                    <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">latest_checkpoint</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># to handle creation afterwards</span>
                <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint_0&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">checkpoint_path</span>

    <span class="k">def</span> <span class="nf">_get_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optimizer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get optimizer from either module or trainer.&quot;&quot;&quot;</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">get_optimizer</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">optimizer</span>
            <span class="n">fused_available</span> <span class="o">=</span> <span class="s2">&quot;fused&quot;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
            <span class="n">optim_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">optim_kwargs</span>
            <span class="n">optim_kwargs</span><span class="p">[</span><span class="s2">&quot;fused&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fused_available</span> <span class="ow">and</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span>
            <span class="n">filtered_kwargs</span> <span class="o">=</span> <span class="n">filter_kwargs</span><span class="p">(</span><span class="n">optim_kwargs</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>

            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">filtered_kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">optimizer</span>

    <span class="k">def</span> <span class="nf">_get_scheduler</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LRScheduler</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get scheduler from either module or trainer.&quot;&quot;&quot;</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">get_scheduler</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">schlr_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">scheduler_kwargs</span>
            <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;last_epoch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;steps_per_epoch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_training_steps</span>
            <span class="n">total_steps</span> <span class="o">=</span> <span class="n">num_training_steps</span> <span class="o">*</span> <span class="n">num_epochs</span>
            <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;num_training_steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_steps</span>
            <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;epochs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_epochs</span>
            <span class="k">if</span> <span class="s2">&quot;num_warmup_steps&quot;</span> <span class="ow">in</span> <span class="n">schlr_kwargs</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;num_warmup_steps&quot;</span><span class="p">],</span> <span class="nb">float</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;num_warmup_steps&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">0.0</span> <span class="ow">or</span> <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;num_warmup_steps&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;If &#39;num_warmup_steps&#39; is a ratio (float value), it needs to be a value between 0 and 1.&quot;</span>
                    <span class="p">)</span>
                <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;num_warmup_steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">total_steps</span> <span class="o">*</span> <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;num_warmup_steps&quot;</span><span class="p">])</span>
            <span class="k">elif</span> <span class="s2">&quot;warmup_ratio&quot;</span> <span class="ow">in</span> <span class="n">schlr_kwargs</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;warmup_ratio&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;&#39;warmup_ratio&#39; value in scheduler configuration needs to be a value between 0 and 1.&quot;</span>
                    <span class="p">)</span>
                <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;num_warmup_steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">total_steps</span> <span class="o">*</span> <span class="n">schlr_kwargs</span><span class="p">[</span><span class="s2">&quot;warmup_ratio&quot;</span><span class="p">])</span>

            <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">scheduler</span>
            <span class="n">filtered_kwargs</span> <span class="o">=</span> <span class="n">filter_kwargs</span><span class="p">(</span><span class="n">schlr_kwargs</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span>

            <span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">filtered_kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scheduler</span>

    <span class="k">def</span> <span class="nf">_get_dataloaders</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">AcceleratorModule</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dataset</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">val_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="n">Dataset</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get DataLoaders for training and validation. Validation dataloaders will be wrapped in a dictionary.&quot;&quot;&quot;</span>
        <span class="n">is_tuple</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="s2">&quot;__len__&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_tuple</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;&#39;batch_size&#39; in hyper parameters needs to be an integer value or a tuple with 2 values &quot;</span>
                <span class="s2">&quot;(one for training and the other for validation).&quot;</span>
            <span class="p">)</span>

        <span class="n">train_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">batch_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">is_tuple</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="n">val_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">batch_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">is_tuple</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="n">dl_args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader_pin_memory</span><span class="p">,</span>
            <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader_num_workers</span><span class="p">,</span>
            <span class="s2">&quot;drop_last&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader_drop_last</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">get_train_dataloader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">train_dataloader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">train_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;Either &#39;train_dataset&#39; or &#39;get_train_dataloader&#39; must be given.&quot;</span>
        <span class="p">)</span>

        <span class="c1"># ignoring &#39;train_dataset&#39; if &#39;get_train_dataloader&#39; was implemented in AcceleratorModule</span>
        <span class="k">if</span> <span class="n">train_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">train_dataloader</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">shuffle_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle_train</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
                <span class="n">train_dataset</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle_train</span><span class="p">,</span>
                <span class="n">sampler</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">samplers</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
                <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collate_fn_train</span><span class="p">,</span>
                <span class="o">**</span><span class="n">dl_args</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">get_validation_dataloader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">val_dataloader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
            <span class="n">val_dataloader</span> <span class="o">=</span> <span class="p">[</span><span class="n">val_dataloader</span><span class="p">]</span>

        <span class="c1"># ignoring &#39;val_dataset&#39; if &#39;get_validation_dataloader&#39; was implemented in AcceleratorModule</span>
        <span class="k">if</span> <span class="n">val_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">val_dataloader</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">val_dataset</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">val_dataset</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="p">{</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">ds</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ds</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">)}</span>
            <span class="p">)</span>
            <span class="n">val_dataloader</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="n">val_dataset</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">val_dataloader</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
                    <span class="n">dataset</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">val_batch_size</span><span class="p">,</span>
                    <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collate_fn_val</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">dl_args</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span>

    <span class="k">def</span> <span class="nf">_get_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AcceleratorModule</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AcceleratorModule</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get module corresponding to the arguments given.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">AcceleratorModule</span><span class="o">.</span><span class="n">from_hf</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">AcceleratorModule</span><span class="o">.</span><span class="n">from_hf</span><span class="p">(</span><span class="o">*</span><span class="n">module</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">module</span>

    <span class="k">def</span> <span class="nf">_init_trackers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize all trackers along with the training configuration from Hyper Parameters and &#39;additional_tracker_config&#39;.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">log_with</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">logger_type</span><span class="p">]</span>
        <span class="n">track_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">track_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">track_name</span>
        <span class="n">init_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">get_init_kwargs</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">)</span>

        <span class="n">config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;effective_batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">tuple</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">num_processes</span> <span class="k">for</span> <span class="n">batch_size</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">num_processes</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation_steps</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">obj</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;effective_batch_size&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">config</span><span class="p">[</span><span class="s2">&quot;effective_batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">obj</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation_steps</span><span class="p">,</span> <span class="n">obj</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">config</span><span class="p">[</span><span class="s2">&quot;effective_batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">obj</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation_steps</span><span class="p">,</span> <span class="n">obj</span><span class="p">)</span>

        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;grad_accumulation_steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation_steps</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;gradient_checkpointing&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;gradient_checkpointing_kwargs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing_kwargs</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;clip_grad&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_processes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">num_processes</span>

        <span class="n">tracker_config</span> <span class="o">=</span> <span class="n">config</span> <span class="o">|</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tracker_config</span>

        <span class="c1"># register signals to end process safely</span>
        <span class="k">def</span> <span class="nf">end_process</span><span class="p">(</span><span class="n">signum</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">end</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="s2">&quot;KILLED&quot;</span><span class="p">)</span>

            <span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">end_on_exception</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">exc_traceback</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="ne">KeyboardInterrupt</span><span class="p">):</span>
                <span class="n">sys</span><span class="o">.</span><span class="n">__excepthook__</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">exc_traceback</span><span class="p">)</span>
                <span class="k">return</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">end</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="s2">&quot;FAILED&quot;</span><span class="p">)</span>
            <span class="n">traceback</span><span class="o">.</span><span class="n">print_exception</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">exc_traceback</span><span class="p">)</span>

        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">,</span> <span class="n">end_process</span><span class="p">)</span>
        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGINT</span><span class="p">,</span> <span class="n">end_process</span><span class="p">)</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">excepthook</span> <span class="o">=</span> <span class="n">end_on_exception</span>

        <span class="k">if</span> <span class="n">MASTER_PROCESS</span><span class="p">:</span>
            <span class="c1"># TODO with a Tracker Wrapper this should be fixed.</span>
            <span class="n">_is_url</span> <span class="o">=</span> <span class="n">is_url</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logging_dir</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">_is_url</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logging</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cannot log results in &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">logging_dir</span><span class="si">}</span><span class="s2">&#39; because &#39;log_with&#39; was not declared.&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">init_trackers</span><span class="p">(</span><span class="n">track_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">tracker_config</span><span class="p">,</span> <span class="n">init_kwargs</span><span class="o">=</span><span class="n">init_kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">set_tracking_uri</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logging_dir</span><span class="p">)</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">run_id</span>

    <span class="k">def</span> <span class="nf">_get_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculates grad norm of model.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">distributed_type</span> <span class="o">==</span> <span class="n">DistributedType</span><span class="o">.</span><span class="n">DEEPSPEED</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wrapped_model</span><span class="o">.</span><span class="n">get_global_grad_norm</span><span class="p">()</span>

        <span class="n">total_norm</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">unwrapped_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">total_norm</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span> <span class="o">**</span> <span class="n">norm_type</span>

        <span class="k">return</span> <span class="n">total_norm</span> <span class="o">**</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span>

<div class="viewcode-block" id="Trainer.log_artifact">
<a class="viewcode-back" href="../../api.html#accmt.Trainer.log_artifact">[docs]</a>
    <span class="k">def</span> <span class="nf">log_artifact</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Logs an artifact to the current run.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (`str`):</span>
<span class="sd">                Path to the file to be logged as an artifact.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logging</span> <span class="ow">and</span> <span class="n">DEBUG_MODE</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="n">path</span><span class="p">)</span></div>


<div class="viewcode-block" id="Trainer.log_artifacts">
<a class="viewcode-back" href="../../api.html#accmt.Trainer.log_artifacts">[docs]</a>
    <span class="k">def</span> <span class="nf">log_artifacts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Logs multiple artifacts from a directory to the current run.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (`str`):</span>
<span class="sd">                Path to the directory to be logged as an artifact.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logging</span> <span class="ow">and</span> <span class="n">DEBUG_MODE</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tracker_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tracker</span><span class="o">.</span><span class="n">log_artifacts</span><span class="p">(</span><span class="n">path</span><span class="p">)</span></div>


    <span class="k">def</span> <span class="nf">_prepare_metrics</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Metric</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Metric</span><span class="p">],</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Metric</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Metric</span><span class="p">]]]],</span>
        <span class="n">val_dataloader</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Metric</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prepare metrics in relation to validation datasets, running checks for types and fixing them if possible.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">Metric</span><span class="p">):</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="p">[</span><span class="n">metrics</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">val_dataloader</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">metrics</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">val_dataloader</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">k</span> <span class="ow">in</span> <span class="n">val_dataloader</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">),</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;There is a mismatch between given metrics and validation datasets. Got </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;for &#39;metrics&#39; and </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">val_dataloader</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2"> for validation datasets.&quot;</span>
            <span class="p">)</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="p">(</span><span class="n">v</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="p">[</span><span class="n">v</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="k">return</span> <span class="n">metrics</span>

<div class="viewcode-block" id="Trainer.register_model_saving">
<a class="viewcode-back" href="../../api.html#accmt.Trainer.register_model_saving">[docs]</a>
    <span class="k">def</span> <span class="nf">register_model_saving</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model_saving</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">saving_below</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">saving_above</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Register a type of model saving.</span>

<span class="sd">        Args:</span>
<span class="sd">            model_saving (`str`):</span>
<span class="sd">                Type of model saving. It can be `&quot;best_valid_loss&quot;` (default), `&quot;best_train_loss&quot;` or in format of</span>
<span class="sd">                `&quot;best_{METRIC}&quot;`. **NOTE**: `&quot;best_&quot;` is optional. Also, all metrics should relate directly to metrics</span>
<span class="sd">                and validation datasets. This can also be in the form of `&quot;best_{METRIC}@{DATASET}&quot;` (metric at a specific dataset),</span>
<span class="sd">                `&quot;best_{METRIC}@{DATASET1}@{DATASET2}&quot;` (metric at dataset1 and dataset2), `&quot;best_{METRIC1}@{DATASET1}/{METRIC1}@{dataset2}&quot;`</span>
<span class="sd">                (best metric1 at dataset1 and best metric2 at dataset2), `&quot;best_{METRIC1}/{METRIC2}@{DATASET2}&quot;` (best metric1 between all</span>
<span class="sd">                datasets containing this metric and best metric2 at dataset2 only), etc.</span>
<span class="sd">            saving_below (`float`, *optional*, defaults to `None`):</span>
<span class="sd">                Register this model saving to only be saved whenever its values are lower than this.</span>
<span class="sd">            saving_above (`float`, *optional*, defaults to `None`):</span>
<span class="sd">                Register this model saving to only be saved whenever its values are above than this.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">saving_below</span> <span class="o">=</span> <span class="n">saving_below</span> <span class="k">if</span> <span class="n">saving_below</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
        <span class="n">saving_above</span> <span class="o">=</span> <span class="n">saving_above</span> <span class="k">if</span> <span class="n">saving_above</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)</span>

        <span class="n">model_saving</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;best_</span><span class="si">{</span><span class="n">model_saving</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">model_saving</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;best_&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">model_saving</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model_saving</span><span class="p">[</span><span class="n">model_saving</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">saving_below</span><span class="p">,</span> <span class="n">saving_above</span><span class="p">)</span></div>


    <span class="k">def</span> <span class="nf">_get_comparator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get comparator for a given metric.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">metrics</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">_metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="n">_metric</span><span class="o">.</span><span class="n">main_metric</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">_metric</span><span class="o">.</span><span class="n">comparator</span>

        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No comparator was found for metric &#39;</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, ghanvert.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>